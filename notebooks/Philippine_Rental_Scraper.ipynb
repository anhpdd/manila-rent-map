{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philippine Rental Property Scraper - Lamudi.com.ph\n",
    "\n",
    "This notebook scrapes rental property listings from Lamudi Philippines for areas within commuting distance of Ortigas Center, Mandaluyong.\n",
    "\n",
    "## Target Criteria:\n",
    "- **Location**: Mandaluyong, Pasig, San Juan, Quezon City, Makati, Taguig\n",
    "- **Property Types**: Condos and Apartments\n",
    "- **Budget**: Under ‚Ç±20,000/month\n",
    "- **Bedrooms**: Studio and 1BR (for solo living)\n",
    "- **Furnishing**: Furnished units\n",
    "\n",
    "## Workflow:\n",
    "1. **Phase 1**: Scrape property listing links from search pages\n",
    "2. **Phase 2**: Extract detailed information from individual property pages\n",
    "3. **Phase 3**: Clean and process the data\n",
    "4. **Phase 4**: Export to CSV for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# BeautifulSoup for HTML parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration: Define Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== USER CONFIGURATION ==========\n",
    "\n",
    "# Target cities (within 1 hour commute to Ortigas Center)\n",
    "CITIES = [\n",
    "    'mandaluyong',    # 0-5 min from Ortigas\n",
    "    'pasig',          # 0-10 min from Ortigas\n",
    "    'san-juan',       # 5-15 min from Ortigas\n",
    "    'quezon-city',    # 10-30 min from Ortigas\n",
    "    'makati',         # 20-35 min from Ortigas\n",
    "    'taguig',         # 20-40 min from Ortigas\n",
    "]\n",
    "\n",
    "# Property types\n",
    "PROPERTY_TYPES = [\n",
    "    'condo',\n",
    "    'apartment',\n",
    "]\n",
    "\n",
    "# Bedroom options (for solo living)\n",
    "BEDROOMS = ['studio', '1-bedroom']\n",
    "\n",
    "# Budget constraint\n",
    "MAX_PRICE = 20000  # PHP per month\n",
    "\n",
    "# Region (Metro Manila)\n",
    "REGION = 'metro-manila'\n",
    "\n",
    "# Output path for CSV files\n",
    "OUTPUT_PATH = 'C:/Users/anhpd/OneDrive/Desktop/projects/phillipine-rental-price/data'\n",
    "\n",
    "# Scraping settings\n",
    "MAX_WORKERS = 3  # Reduced from 5 to be more conservative\n",
    "HEADLESS = True  # Set to False to see browser during scraping\n",
    "PAGE_LOAD_WAIT = 4  # Seconds to wait for page to load\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded!\")\n",
    "print(f\"   üìç Cities: {', '.join(CITIES)}\")\n",
    "print(f\"   üè¢ Property Types: {', '.join(PROPERTY_TYPES)}\")\n",
    "print(f\"   üí∞ Max Budget: ‚Ç±{MAX_PRICE:,}/month\")\n",
    "print(f\"   üõèÔ∏è  Bedrooms: {', '.join(BEDROOMS)}\")\n",
    "print(f\"   üë∑ Workers: {MAX_WORKERS}\")\n",
    "print(f\"   üìÅ Output: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell: Inspect Lamudi Page Structure\n",
    "\n",
    "**Run this first** to identify the correct CSS selectors for Lamudi's current HTML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_lamudi_structure(city='pasig', prop_type='condo'):\n",
    "    \"\"\"\n",
    "    Opens a Lamudi search page and inspects its HTML structure.\n",
    "    This helps us identify the correct CSS selectors.\n",
    "    \"\"\"\n",
    "    url = f'https://www.lamudi.com.ph/rent/{REGION}/{city}/{prop_type}/'\n",
    "    print(f\"üîç Inspecting: {url}\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        # Setup Chrome options\n",
    "        chrome_options = Options()\n",
    "        if HEADLESS:\n",
    "            chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(PAGE_LOAD_WAIT)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        print(\"\\nüìÑ Page Title:\", driver.title)\n",
    "        print(\"\\nüîç Looking for common patterns...\\n\")\n",
    "        \n",
    "        # Try to find listing containers\n",
    "        possible_containers = [\n",
    "            soup.find_all('div', class_=re.compile(r'listing|property|card|item', re.I)),\n",
    "            soup.find_all('article'),\n",
    "            soup.find_all('a', href=re.compile(r'/.*/'))\n",
    "        ]\n",
    "        \n",
    "        print(\"üì¶ Found potential listing containers:\")\n",
    "        for i, containers in enumerate(possible_containers):\n",
    "            print(f\"   Pattern {i+1}: {len(containers)} elements\")\n",
    "        \n",
    "        # Look for links that might be property links\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        property_links = [link for link in all_links if '/rent/' in link.get('href', '')]\n",
    "        \n",
    "        print(f\"\\nüîó Found {len(property_links)} links containing '/rent/'\")\n",
    "        \n",
    "        if property_links:\n",
    "            print(\"\\nüìã Sample property links:\")\n",
    "            for link in property_links[:3]:\n",
    "                print(f\"   - {link.get('href')}\")\n",
    "                print(f\"     Classes: {link.get('class')}\")\n",
    "                print(f\"     Data attributes: {[k for k in link.attrs.keys() if k.startswith('data-')]}\")\n",
    "        \n",
    "        # Look for pagination\n",
    "        pagination = soup.find_all(['nav', 'div'], class_=re.compile(r'paginat', re.I))\n",
    "        print(f\"\\nüìÑ Pagination elements found: {len(pagination)}\")\n",
    "        \n",
    "        # Save sample HTML for manual inspection\n",
    "        with open(f'{OUTPUT_PATH}/sample_page.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(soup.prettify())\n",
    "        print(f\"\\nüíæ Full HTML saved to: {OUTPUT_PATH}/sample_page.html\")\n",
    "        \n",
    "        return soup\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# Run the inspection\n",
    "print(\"üöÄ Starting inspection...\\n\")\n",
    "soup = inspect_lamudi_structure()\n",
    "print(\"\\n‚úÖ Inspection complete! Check the output above to identify CSS selectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions: Page Range Detection and Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_range(city, prop_type):\n",
    "    \"\"\"\n",
    "    Detects the number of pages available for a given city/property type combination.\n",
    "    Returns (min_page, max_page)\n",
    "    \"\"\"\n",
    "    url = f'https://www.lamudi.com.ph/rent/{REGION}/{city}/{prop_type}/'\n",
    "    print(f\"üîç Detecting page range for: {city}/{prop_type}\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        if HEADLESS:\n",
    "            chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(PAGE_LOAD_WAIT)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # TODO: Update these selectors based on inspection results\n",
    "        # Look for pagination elements\n",
    "        pagination = soup.find('nav', class_=re.compile(r'paginat', re.I))\n",
    "        \n",
    "        if not pagination:\n",
    "            print(\"   ‚ö†Ô∏è  No pagination found, assuming single page\")\n",
    "            return (1, 1)\n",
    "        \n",
    "        # Extract page numbers from pagination\n",
    "        page_links = pagination.find_all('a', href=True)\n",
    "        page_numbers = []\n",
    "        \n",
    "        for link in page_links:\n",
    "            # Try to extract page number from href (e.g., /rent/metro-manila/pasig/condo/?page=2)\n",
    "            match = re.search(r'page=(\\d+)', link.get('href', ''))\n",
    "            if match:\n",
    "                page_numbers.append(int(match.group(1)))\n",
    "        \n",
    "        if page_numbers:\n",
    "            min_page = 1  # Always start from page 1\n",
    "            max_page = max(page_numbers)\n",
    "            print(f\"   ‚úÖ Pages: 1 to {max_page}\")\n",
    "            return (min_page, max_page)\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Could not parse page numbers, assuming single page\")\n",
    "            return (1, 1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return (1, 1)\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "def extract_links_from_soup(soup, city, prop_type):\n",
    "    \"\"\"\n",
    "    Extracts property links and IDs from a BeautifulSoup object.\n",
    "    Returns a DataFrame with columns: property_id, url, city, property_type\n",
    "    \"\"\"\n",
    "    # TODO: Update these selectors based on inspection results\n",
    "    # Look for property listing links\n",
    "    \n",
    "    # Try multiple patterns to find property links\n",
    "    property_links = []\n",
    "    \n",
    "    # Pattern 1: Links with specific data attributes (adjust based on inspection)\n",
    "    links_with_data = soup.find_all('a', attrs={'data-property-id': True})\n",
    "    property_links.extend(links_with_data)\n",
    "    \n",
    "    # Pattern 2: Links in listing containers (adjust class name based on inspection)\n",
    "    listings = soup.find_all(['div', 'article'], class_=re.compile(r'listing|property|card', re.I))\n",
    "    for listing in listings:\n",
    "        link = listing.find('a', href=True)\n",
    "        if link and '/rent/' in link.get('href', ''):\n",
    "            property_links.append(link)\n",
    "    \n",
    "    # Pattern 3: All links containing '/rent/' in href\n",
    "    if not property_links:\n",
    "        all_links = soup.find_all('a', href=re.compile(r'/rent/.*'))\n",
    "        property_links.extend(all_links)\n",
    "    \n",
    "    # Extract data\n",
    "    data = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    for link in property_links:\n",
    "        href = link.get('href', '')\n",
    "        \n",
    "        # Skip if not a full property URL\n",
    "        if not href or href in seen_urls:\n",
    "            continue\n",
    "        \n",
    "        # Make absolute URL if needed\n",
    "        if href.startswith('/'):\n",
    "            href = f'https://www.lamudi.com.ph{href}'\n",
    "        \n",
    "        # Extract property ID from URL or data attribute\n",
    "        prop_id = link.get('data-property-id') or link.get('id') or re.search(r'/(\\d+)/?$', href)\n",
    "        if prop_id and hasattr(prop_id, 'group'):\n",
    "            prop_id = prop_id.group(1)\n",
    "        \n",
    "        data.append({\n",
    "            'property_id': str(prop_id) if prop_id else None,\n",
    "            'url': href,\n",
    "            'city': city,\n",
    "            'property_type': prop_type\n",
    "        })\n",
    "        seen_urls.add(href)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Scrape Property Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_from_page(city, prop_type, page_number):\n",
    "    \"\"\"\n",
    "    Scrapes property links from a single search results page.\n",
    "    \"\"\"\n",
    "    # Build URL with page number\n",
    "    if page_number == 1:\n",
    "        url = f'https://www.lamudi.com.ph/rent/{REGION}/{city}/{prop_type}/'\n",
    "    else:\n",
    "        url = f'https://www.lamudi.com.ph/rent/{REGION}/{city}/{prop_type}/?page={page_number}'\n",
    "    \n",
    "    print(f\"üöÄ Scraping: {city}/{prop_type} - Page {page_number}\")\n",
    "    \n",
    "    driver = None\n",
    "    max_retries = 2\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            if HEADLESS:\n",
    "                chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--log-level=3')\n",
    "            chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "            chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "            \n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "            driver.get(url)\n",
    "            time.sleep(PAGE_LOAD_WAIT)\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            df = extract_links_from_soup(soup, city, prop_type)\n",
    "            \n",
    "            if not df.empty:\n",
    "                print(f\"   ‚úÖ Found {len(df)} properties\")\n",
    "                return df\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  No properties found\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            retry_count += 1\n",
    "            print(f\"   ‚ùå Error (attempt {retry_count}/{max_retries + 1}): {e}\")\n",
    "            if retry_count <= max_retries:\n",
    "                print(f\"   üîÑ Retrying...\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                return pd.DataFrame()\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Link scraping function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Phase 1: Collect All Property Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: COLLECTING PROPERTY LINKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Detect page ranges for all city/property combinations\n",
    "print(\"\\nüîç Step 1: Detecting page ranges...\\n\")\n",
    "task_ranges = {}\n",
    "\n",
    "for city in CITIES:\n",
    "    for prop_type in PROPERTY_TYPES:\n",
    "        min_page, max_page = get_page_range(city, prop_type)\n",
    "        task_ranges[(city, prop_type)] = (min_page, max_page)\n",
    "        time.sleep(1)  # Brief pause between page range checks\n",
    "\n",
    "# Step 2: Create list of all scraping tasks\n",
    "print(\"\\nüìã Step 2: Building task list...\\n\")\n",
    "tasks = []\n",
    "\n",
    "for (city, prop_type), (min_page, max_page) in task_ranges.items():\n",
    "    for page_num in range(min_page, max_page + 1):\n",
    "        tasks.append((city, prop_type, page_num))\n",
    "\n",
    "# Randomize task order to avoid patterns\n",
    "random.shuffle(tasks)\n",
    "print(f\"‚úÖ Created {len(tasks)} scraping tasks (randomized)\\n\")\n",
    "\n",
    "# Step 3: Execute tasks concurrently\n",
    "print(\"\\nüèÉ Step 3: Scraping links...\\n\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    results = executor.map(lambda p: scrape_links_from_page(*p), tasks)\n",
    "    df_list = [df for df in results if df is not None and not df.empty]\n",
    "\n",
    "# Step 4: Combine and deduplicate\n",
    "print(\"\\nüîó Step 4: Combining results...\\n\")\n",
    "\n",
    "if df_list:\n",
    "    links_df = pd.concat(df_list, ignore_index=True)\n",
    "    rows_before = len(links_df)\n",
    "    links_df = links_df.drop_duplicates(subset=['url']).reset_index(drop=True)\n",
    "    rows_after = len(links_df)\n",
    "    duplicates_removed = rows_before - rows_after\n",
    "    \n",
    "    print(f\"‚úÖ Combined {len(df_list)} result sets\")\n",
    "    print(f\"üßπ Removed {duplicates_removed} duplicates\")\n",
    "    print(f\"üìä Total unique properties: {rows_after}\")\n",
    "    \n",
    "    # Show breakdown by city and property type\n",
    "    print(\"\\nüìç Breakdown by location and type:\")\n",
    "    for city in CITIES:\n",
    "        city_total = len(links_df[links_df['city'] == city])\n",
    "        if city_total > 0:\n",
    "            print(f\"\\n   {city.upper()}: {city_total} properties\")\n",
    "            for prop_type in PROPERTY_TYPES:\n",
    "                count = len(links_df[(links_df['city'] == city) & (links_df['property_type'] == prop_type)])\n",
    "                if count > 0:\n",
    "                    print(f\"      - {prop_type}: {count}\")\n",
    "    \n",
    "    # Save links to CSV\n",
    "    links_file = f\"{OUTPUT_PATH}/property_links_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    links_df.to_csv(links_file, index=False)\n",
    "    print(f\"\\nüíæ Links saved to: {links_file}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No links were collected\")\n",
    "    links_df = pd.DataFrame()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è  Phase 1 completed in {elapsed:.2f} seconds\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Scrape Detailed Property Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_property_details(url):\n",
    "    \"\"\"\n",
    "    Scrapes detailed information from a single property listing page.\n",
    "    Returns a single-row DataFrame with all extracted data.\n",
    "    \"\"\"\n",
    "    print(f\"üè† Scraping: {url}\")\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        chrome_options = Options()\n",
    "        if HEADLESS:\n",
    "            chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        \n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "        driver.get(url)\n",
    "        time.sleep(PAGE_LOAD_WAIT)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # Helper function to safely extract text\n",
    "        def get_text(selector, attrs=None, default=None):\n",
    "            element = soup.find(selector, attrs=attrs) if attrs else soup.find(selector)\n",
    "            return element.get_text(strip=True) if element else default\n",
    "        \n",
    "        # TODO: Update these selectors based on actual Lamudi structure\n",
    "        # You'll need to inspect the property detail pages to find the correct selectors\n",
    "        \n",
    "        data = {\n",
    "            'url': url,\n",
    "            'title': get_text('h1'),  # Adjust selector\n",
    "            'price': None,  # Will be extracted and parsed below\n",
    "            'bedrooms': None,\n",
    "            'bathrooms': None,\n",
    "            'floor_area_sqm': None,\n",
    "            'furnishing': None,\n",
    "            'address': None,\n",
    "            'location': None,\n",
    "            'description': None,\n",
    "            'latitude': None,\n",
    "            'longitude': None,\n",
    "            'amenities': None,\n",
    "            'parking': None,\n",
    "        }\n",
    "        \n",
    "        # Extract price (look for PHP symbol or pattern)\n",
    "        price_elem = soup.find(text=re.compile(r'‚Ç±|PHP'))\n",
    "        if price_elem:\n",
    "            price_match = re.search(r'[‚Ç±PHP\\s]*([\\d,]+)', str(price_elem))\n",
    "            if price_match:\n",
    "                data['price'] = price_match.group(1).replace(',', '')\n",
    "        \n",
    "        # Extract specifications (bedrooms, bathrooms, area)\n",
    "        # Look for common patterns in property specs\n",
    "        specs = soup.find_all(['div', 'span', 'li'], class_=re.compile(r'spec|feature|detail', re.I))\n",
    "        for spec in specs:\n",
    "            text = spec.get_text(strip=True).lower()\n",
    "            \n",
    "            if 'bedroom' in text:\n",
    "                match = re.search(r'(\\d+)', text)\n",
    "                if match:\n",
    "                    data['bedrooms'] = match.group(1)\n",
    "            \n",
    "            if 'bathroom' in text or 'bath' in text:\n",
    "                match = re.search(r'(\\d+)', text)\n",
    "                if match:\n",
    "                    data['bathrooms'] = match.group(1)\n",
    "            \n",
    "            if 'sqm' in text or 'm¬≤' in text or 'floor area' in text:\n",
    "                match = re.search(r'([\\d,\\.]+)', text)\n",
    "                if match:\n",
    "                    data['floor_area_sqm'] = match.group(1).replace(',', '')\n",
    "            \n",
    "            if 'furnish' in text:\n",
    "                data['furnishing'] = text\n",
    "            \n",
    "            if 'parking' in text:\n",
    "                data['parking'] = text\n",
    "        \n",
    "        # Extract coordinates from script tags\n",
    "        scripts = soup.find_all('script')\n",
    "        for script in scripts:\n",
    "            if script.string and ('latitude' in script.string or 'lat' in script.string):\n",
    "                lat_match = re.search(r'[\"\\']?lat(?:itude)?[\"\\']?\\s*[:=]\\s*([\\d\\.\\-]+)', script.string)\n",
    "                lon_match = re.search(r'[\"\\']?lon(?:g|gitude)?[\"\\']?\\s*[:=]\\s*([\\d\\.\\-]+)', script.string)\n",
    "                \n",
    "                if lat_match and lon_match:\n",
    "                    data['latitude'] = lat_match.group(1)\n",
    "                    data['longitude'] = lon_match.group(1)\n",
    "                    break\n",
    "        \n",
    "        # Extract description\n",
    "        desc_elem = soup.find(['div', 'p'], class_=re.compile(r'description|detail', re.I))\n",
    "        if desc_elem:\n",
    "            data['description'] = desc_elem.get_text(strip=True)[:500]  # Limit length\n",
    "        \n",
    "        # Extract address/location\n",
    "        addr_elem = soup.find(['div', 'span', 'p'], class_=re.compile(r'address|location', re.I))\n",
    "        if addr_elem:\n",
    "            data['address'] = addr_elem.get_text(strip=True)\n",
    "        \n",
    "        print(f\"   ‚úÖ Extracted: {data.get('title', 'Unknown')[:50]}...\")\n",
    "        \n",
    "        # Return as single-row DataFrame\n",
    "        return pd.DataFrame([data])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Property details scraping function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Phase 2: Collect Property Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: COLLECTING PROPERTY DETAILS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if links_df.empty:\n",
    "    print(\"‚ö†Ô∏è  No links available. Please run Phase 1 first.\")\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get list of URLs to scrape\n",
    "    urls_to_scrape = links_df['url'].tolist()\n",
    "    print(f\"üìã Will scrape {len(urls_to_scrape)} property pages\\n\")\n",
    "    \n",
    "    # Execute scraping with thread pool\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        results = executor.map(scrape_property_details, urls_to_scrape)\n",
    "        details_list = [df for df in results if df is not None and not df.empty]\n",
    "    \n",
    "    # Combine results\n",
    "    if details_list:\n",
    "        details_df = pd.concat(details_list, ignore_index=True)\n",
    "        print(f\"\\n‚úÖ Successfully scraped {len(details_df)} properties\")\n",
    "        \n",
    "        # Merge with links data\n",
    "        final_df = pd.merge(details_df, links_df, on='url', how='left')\n",
    "        \n",
    "        # Save raw details\n",
    "        raw_file = f\"{OUTPUT_PATH}/property_details_raw_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_df.to_csv(raw_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"üíæ Raw details saved to: {raw_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No property details were collected\")\n",
    "        final_df = pd.DataFrame()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è  Phase 2 completed in {elapsed:.2f} seconds\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3: DATA CLEANING AND FILTERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'final_df' not in locals() or final_df.empty:\n",
    "    print(\"‚ö†Ô∏è  No data available. Please run Phase 2 first.\")\n",
    "else:\n",
    "    print(f\"\\nüìä Starting with {len(final_df)} properties\\n\")\n",
    "    \n",
    "    # Create cleaned dataframe\n",
    "    cleaned_df = final_df.copy()\n",
    "    \n",
    "    # 1. Clean and convert price\n",
    "    print(\"üí∞ Step 1: Cleaning price data...\")\n",
    "    cleaned_df['price_php'] = pd.to_numeric(\n",
    "        cleaned_df['price'].astype(str).str.replace(',', '').str.replace('[^0-9]', '', regex=True),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # 2. Filter by budget (under ‚Ç±20,000/month)\n",
    "    print(f\"üîç Step 2: Filtering by budget (under ‚Ç±{MAX_PRICE:,}/month)...\")\n",
    "    before_filter = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df[\n",
    "        (cleaned_df['price_php'] > 0) & \n",
    "        (cleaned_df['price_php'] <= MAX_PRICE)\n",
    "    ].copy()\n",
    "    after_filter = len(cleaned_df)\n",
    "    print(f\"   Removed {before_filter - after_filter} properties outside budget\")\n",
    "    print(f\"   Remaining: {after_filter} properties\")\n",
    "    \n",
    "    # 3. Filter by furnishing (furnished only)\n",
    "    print(\"üõãÔ∏è  Step 3: Filtering for furnished properties...\")\n",
    "    before_filter = len(cleaned_df)\n",
    "    cleaned_df = cleaned_df[\n",
    "        cleaned_df['furnishing'].astype(str).str.contains('furnished', case=False, na=False)\n",
    "    ].copy()\n",
    "    after_filter = len(cleaned_df)\n",
    "    print(f\"   Removed {before_filter - after_filter} unfurnished properties\")\n",
    "    print(f\"   Remaining: {after_filter} properties\")\n",
    "    \n",
    "    # 4. Clean numeric fields\n",
    "    print(\"üî¢ Step 4: Converting numeric fields...\")\n",
    "    cleaned_df['bedrooms'] = pd.to_numeric(cleaned_df['bedrooms'], errors='coerce')\n",
    "    cleaned_df['bathrooms'] = pd.to_numeric(cleaned_df['bathrooms'], errors='coerce')\n",
    "    cleaned_df['floor_area_sqm'] = pd.to_numeric(\n",
    "        cleaned_df['floor_area_sqm'].astype(str).str.replace(',', ''),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    cleaned_df['latitude'] = pd.to_numeric(cleaned_df['latitude'], errors='coerce')\n",
    "    cleaned_df['longitude'] = pd.to_numeric(cleaned_df['longitude'], errors='coerce')\n",
    "    \n",
    "    # 5. Calculate price per sqm\n",
    "    print(\"üìê Step 5: Calculating price per sqm...\")\n",
    "    cleaned_df['price_per_sqm'] = (\n",
    "        cleaned_df['price_php'] / cleaned_df['floor_area_sqm']\n",
    "    ).round(2)\n",
    "    \n",
    "    # 6. Add commute time estimates (based on city)\n",
    "    print(\"üöá Step 6: Adding commute time estimates...\")\n",
    "    commute_times = {\n",
    "        'mandaluyong': '0-5 min',\n",
    "        'pasig': '0-10 min',\n",
    "        'san-juan': '5-15 min',\n",
    "        'quezon-city': '10-30 min',\n",
    "        'makati': '20-35 min',\n",
    "        'taguig': '20-40 min',\n",
    "    }\n",
    "    cleaned_df['commute_estimate'] = cleaned_df['city'].map(commute_times)\n",
    "    \n",
    "    # 7. Reorder and select important columns\n",
    "    print(\"üìã Step 7: Organizing columns...\")\n",
    "    column_order = [\n",
    "        'title', 'price_php', 'bedrooms', 'bathrooms', 'floor_area_sqm', 'price_per_sqm',\n",
    "        'furnishing', 'city', 'property_type', 'commute_estimate',\n",
    "        'address', 'latitude', 'longitude',\n",
    "        'parking', 'amenities', 'description', 'url'\n",
    "    ]\n",
    "    \n",
    "    # Only include columns that exist\n",
    "    column_order = [col for col in column_order if col in cleaned_df.columns]\n",
    "    cleaned_df = cleaned_df[column_order]\n",
    "    \n",
    "    # 8. Sort by price and commute time\n",
    "    print(\"üìä Step 8: Sorting results...\")\n",
    "    cleaned_df = cleaned_df.sort_values(['city', 'price_php']).reset_index(drop=True)\n",
    "    \n",
    "    # 9. Display summary statistics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nüìä Total properties matching criteria: {len(cleaned_df)}\")\n",
    "    \n",
    "    if len(cleaned_df) > 0:\n",
    "        print(f\"\\nüí∞ Price Range:\")\n",
    "        print(f\"   Minimum: ‚Ç±{cleaned_df['price_php'].min():,.0f}/month\")\n",
    "        print(f\"   Average: ‚Ç±{cleaned_df['price_php'].mean():,.0f}/month\")\n",
    "        print(f\"   Maximum: ‚Ç±{cleaned_df['price_php'].max():,.0f}/month\")\n",
    "        \n",
    "        print(f\"\\nüìç Properties by City:\")\n",
    "        for city in CITIES:\n",
    "            count = len(cleaned_df[cleaned_df['city'] == city])\n",
    "            if count > 0:\n",
    "                avg_price = cleaned_df[cleaned_df['city'] == city]['price_php'].mean()\n",
    "                print(f\"   {city.title()}: {count} properties (avg: ‚Ç±{avg_price:,.0f}/month)\")\n",
    "        \n",
    "        print(f\"\\nüè¢ Properties by Type:\")\n",
    "        for prop_type in PROPERTY_TYPES:\n",
    "            count = len(cleaned_df[cleaned_df['property_type'] == prop_type])\n",
    "            if count > 0:\n",
    "                print(f\"   {prop_type.title()}: {count} properties\")\n",
    "        \n",
    "        # 10. Save cleaned data\n",
    "        print(\"\\nüíæ Saving cleaned data...\")\n",
    "        output_file = f\"{OUTPUT_PATH}/ortigas_rentals_under_20k_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        cleaned_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   ‚úÖ Saved to: {output_file}\")\n",
    "        \n",
    "        # Display sample of best deals\n",
    "        print(\"\\nüåü Top 10 Best Deals (Lowest Price):\")\n",
    "        print(cleaned_df[['title', 'price_php', 'city', 'bedrooms', 'floor_area_sqm', 'commute_estimate']].head(10).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Phase 3 Complete!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cleaned dataframe\n",
    "if 'cleaned_df' in locals() and not cleaned_df.empty:\n",
    "    display(cleaned_df.head(20))\n",
    "else:\n",
    "    print(\"No data available. Please run all phases first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After running this notebook:\n",
    "\n",
    "1. **Review the data**: Check the CSV file for quality and completeness\n",
    "2. **Adjust CSS selectors**: If data extraction is incomplete, update the selectors based on browser inspection\n",
    "3. **Refine filters**: Adjust budget, cities, or property types as needed\n",
    "4. **Schedule regular runs**: Run periodically to track new listings\n",
    "5. **Data analysis**: Import the CSV into your preferred analysis tool\n",
    "\n",
    "**Important Notes:**\n",
    "- Always respect the website's Terms of Service and robots.txt\n",
    "- Use appropriate rate limiting to avoid overloading servers\n",
    "- Website structures change - selectors may need periodic updates\n",
    "- Verify extracted data accuracy before making decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
